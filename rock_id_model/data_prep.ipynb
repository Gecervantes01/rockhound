{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "This notebook handles the preprocessing of the dataset that is used to train the model.\n",
    "<br><br>\n",
    "\n",
    "### __Dataset Details__\n",
    "The data consists of labeled images of rocks and minerals organized by their respective classes: Igneous, Sedimentary, and Metamorphic. <br><br>\n",
    "There are two sets of data, the [__training data__](#training-data-processing) and the [__testing data__](#test-data-processing). Each set follows the same structure outlined above, with the only difference coming from how the images are preprocessed. <br><br>\n",
    "\n",
    "### __Preprocessing Transformations__\n",
    "#### Resizing\n",
    "Image size affects a variety of factors when it comes to training a model, but mainly __performance__ and __detail__. Due to rocks and minerals being the target of identification, i'll be training the model with and image size of __256px x 256px__. This will add more detail for the model to train on, which will hopefully translate to improved accuracy when fed sub-optimal images to identify.\n",
    "<br><br>\n",
    "\n",
    "#### Augmentation\n",
    "Both the `.RandomHorizontalFlip()` and `.RandomRotation()` functions are used to augment the images, providing the model with a greater variety of training data from a limited orignal dataset.\n",
    "<br><br>\n",
    "\n",
    "#### Tensor Conversion\n",
    "...\n",
    "<br><br>\n",
    "\n",
    "#### Normalization\n",
    "...\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Splitting dataset\n",
    "The first step is to split the data into a set for training and a set for testing. I'm going with an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for splitting data into training and testing data\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "dataset_path = 'rock_dataset\\\\'\n",
    "\n",
    "# Arrays storing image paths & labels for use with\n",
    "# train_test_split function\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    # igneous, metamorphic, sedimentary\n",
    "    for subfolder in os.listdir(os.path.join(dataset_path, folder)):\n",
    "        subfolder_path = os.path.join(dataset_path, folder, subfolder)\n",
    "        for rock in os.listdir(subfolder_path):\n",
    "            image_paths.append(os.path.join(subfolder_path, rock))\n",
    "            image_labels.append(subfolder)\n",
    "\n",
    "# Now we split the data up into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_paths, image_labels, test_size=0.2, stratify=image_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Processing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch to preprocess images\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "# Const vairable for image size\n",
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_processing = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data Processing\n",
    "Test data is processed similary to the training data, with the exception of augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_processing = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
